{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5da7bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å¾ AML-1.pdf ä¸­æå–äº† 6 å€‹è¶…é€£çµã€‚\n",
      "ğŸ“ è¼¸å‡ºå·²å„²å­˜è‡³ c:\\Users\\User\\Desktop\\GraphRAG\\Main_Workflow\\Hyperlink_extract\\AML-1.links_normalized.csv\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import csv\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_links_with_dimensions(pdf_path: Path, output_csv_path: Path):\n",
    "    \"\"\"\n",
    "    å¾ PDF ä¸­æå–è¶…é€£çµï¼Œä¸¦åŒ…å«é é¢å°ºå¯¸ä»¥ä¾¿é€²è¡Œæ­£è¦åŒ–ã€‚\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    links_data = []\n",
    "\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        page_width = page.rect.width\n",
    "        page_height = page.rect.height\n",
    "\n",
    "        for link in page.get_links():\n",
    "            uri = link.get(\"uri\")\n",
    "            if not uri:\n",
    "                continue\n",
    "            \n",
    "            rect = link.get(\"from\")\n",
    "            if not rect:\n",
    "                continue\n",
    "\n",
    "            links_data.append({\n",
    "                \"page\": page_num,  # ä½¿ç”¨ 0-based ç´¢å¼•ï¼Œèˆ‡ MinerU å°é½Š\n",
    "                \"page_width\": page_width,\n",
    "                \"page_height\": page_height,\n",
    "                \"uri\": uri,\n",
    "                \"rect\": str(rect),\n",
    "            })\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    if not links_data:\n",
    "        print(f\"âš ï¸ åœ¨ {pdf_path.name} ä¸­æ²’æœ‰æ‰¾åˆ°ä»»ä½•è¶…é€£çµã€‚\")\n",
    "        return\n",
    "\n",
    "    # å¯«å…¥ CSV\n",
    "    output_csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(output_csv_path, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=links_data[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(links_data)\n",
    "\n",
    "    print(f\"âœ… å¾ {pdf_path.name} ä¸­æå–äº† {len(links_data)} å€‹è¶…é€£çµã€‚\")\n",
    "    print(f\"ğŸ“ è¼¸å‡ºå·²å„²å­˜è‡³ {output_csv_path}\")\n",
    "\n",
    "\n",
    "DOC_CODE = \"AML-1\"\n",
    "PDF_PATH = Path(f\"c:/Users/User/Desktop/GraphRAG/policy_dataset/IC-1 Risk Management Framework/{DOC_CODE}.pdf\")\n",
    "OUTPUT_LINKS_CSV = Path(f\"c:/Users/User/Desktop/GraphRAG/Main_Workflow/Hyperlink_extract/{DOC_CODE}.links_normalized.csv\")\n",
    "    \n",
    "extract_links_with_dimensions(PDF_PATH, OUTPUT_LINKS_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350d83af",
   "metadata": {},
   "source": [
    "# Hyperlink Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "104e4277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re, pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0) è·¯å¾‘\n",
    "# ------------------------------------------------------------\n",
    "# DOC_CODE = \"IC-1\"\n",
    "CONTENT_LIST_JSON = Path(f\"C:/Users/User/Desktop/GraphRAG/out_md_mineru/{DOC_CODE}/auto/{DOC_CODE}_content_list.json\")\n",
    "INPUT_LINKS_CSV   = Path(f\"C:/Users/User/Desktop/GraphRAG/Main_Workflow/Hyperlink_extract/{DOC_CODE}.links_normalized.csv\")\n",
    "OUTPUT_MATCH_CSV  = Path(f\"C:/Users/User/Desktop/GraphRAG/Main_Workflow/Hyperlink_extract/{DOC_CODE}_hyperlink_matches_with_clause.csv\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) å»º blocks\n",
    "# ------------------------------------------------------------\n",
    "def load_blocks(content_list_path: Path):\n",
    "    with open(content_list_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = json.load(f)\n",
    "    blocks = []\n",
    "    for i, b in enumerate(raw):\n",
    "        blocks.append({\n",
    "            \"block_idx\": i,\n",
    "            \"type\": b.get(\"type\"),\n",
    "            \"page_idx\": b.get(\"page_idx\"),\n",
    "            \"bbox\": b.get(\"bbox\"),\n",
    "            \"text\": (b.get(\"text\") or b.get(\"table_body\") or \"\").strip()\n",
    "        })\n",
    "    return blocks\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) æ¢æ¬¾è¡Œåµæ¸¬\n",
    "# ------------------------------------------------------------\n",
    "HEADING_RE = re.compile(r'^\\s*(\\d+(?:\\.\\d+)*)[.\\)]?\\s+(.*\\S)?\\s*$')\n",
    "\n",
    "def classify_heading(text: str) -> Optional[Tuple[str, int, str]]:\n",
    "    \"\"\"\n",
    "    å¦‚æœæ˜¯æ¢æ¬¾/ç« ç¯€æ¨™é¡Œï¼Œå›å‚³ (clause_id, level, title)\n",
    "    - level = id çš„æ®µæ•¸ï¼Œå¦‚ \"1\" -> 1, \"1.1\"->2, \"1.1.1\"->3\n",
    "    å¦å‰‡å›å‚³ None\n",
    "    \"\"\"\n",
    "    m = HEADING_RE.match(text)\n",
    "    if not m:\n",
    "        return None\n",
    "    cid = m.group(1)\n",
    "    title = m.group(2) or \"\"\n",
    "    level = cid.count(\".\") + 1\n",
    "    return (cid, level, title.strip())\n",
    "\n",
    "def find_headings(blocks):\n",
    "    heads = []\n",
    "    for b in blocks:\n",
    "        if b[\"type\"] != \"text\":\n",
    "            continue\n",
    "        h = classify_heading(b.get(\"text\",\"\"))\n",
    "        if not h:\n",
    "            continue\n",
    "        cid, level, title = h\n",
    "        heads.append({\n",
    "            \"block_idx\": b[\"block_idx\"],\n",
    "            \"page_idx\": b[\"page_idx\"],\n",
    "            \"clause_id\": cid,\n",
    "            \"level\": level,\n",
    "            \"title\": title\n",
    "        })\n",
    "    heads.sort(key=lambda x: x[\"block_idx\"])\n",
    "    return heads\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) ç”± heading å»º spans + æ¢æ¬¾å…¨æ–‡\n",
    "# ------------------------------------------------------------\n",
    "def slice_body_text(blocks, start_idx, end_idx):\n",
    "    texts = []\n",
    "    for i in range(start_idx, end_idx):\n",
    "        b = blocks[i]\n",
    "        if b[\"type\"] == \"text\":\n",
    "            txt = b.get(\"text\",\"\").strip()\n",
    "            if txt:\n",
    "                texts.append(txt)\n",
    "    return \"\\n\".join(texts).strip()\n",
    "\n",
    "def build_spans_dedup(blocks, heads):\n",
    "    cands = []\n",
    "    for i, h in enumerate(heads):\n",
    "        start = h[\"block_idx\"]\n",
    "        end = heads[i+1][\"block_idx\"] if i+1 < len(heads) else len(blocks)\n",
    "        body_len = len(slice_body_text(blocks, start+1, end))\n",
    "        full_text = slice_body_text(blocks, start, end)   # â˜… æ–°å¢ï¼šå®Œæ•´æ¢æ¬¾æ–‡å­—\n",
    "        cands.append({\n",
    "            \"clause_id\": h[\"clause_id\"],\n",
    "            \"start_block\": start,\n",
    "            \"end_block\": end,\n",
    "            \"page_idx\": h[\"page_idx\"],\n",
    "            \"level\": h[\"level\"],\n",
    "            \"title\": h[\"title\"],\n",
    "            \"body_len\": body_len,\n",
    "            \"full_text\": full_text                        # â˜… æ–°å¢\n",
    "        })\n",
    "    best = {}\n",
    "    for c in cands:\n",
    "        cid = c[\"clause_id\"]\n",
    "        cur = best.get(cid)\n",
    "        if (cur is None) or (c[\"body_len\"] > cur[\"body_len\"]):\n",
    "            best[cid] = c\n",
    "    spans = sorted(best.values(), key=lambda x: x[\"start_block\"])\n",
    "    return spans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ca5cad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 4) è¶…é€£çµ â†’ block\n",
    "# ------------------------------------------------------------\n",
    "def parse_rect_string(rect_str: str):\n",
    "    try:\n",
    "        s = rect_str.strip()\n",
    "        s = re.sub(r'^\\s*Rect\\(\\s*', '', s, flags=re.I)\n",
    "        s = re.sub(r'\\)\\s*$', '', s)\n",
    "        return [float(c.strip()) for c in s.split(\",\")]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def normalize_rect(rect_coords, page_w, page_h):\n",
    "    x0, y0, x1, y1 = rect_coords\n",
    "    return [\n",
    "        (x0 / page_w)  * 1000.0,\n",
    "        (y0 / page_h)  * 1000.0,\n",
    "        (x1 / page_w)  * 1000.0,\n",
    "        (y1 / page_h)  * 1000.0,\n",
    "    ]\n",
    "\n",
    "def is_contained(inner_box, outer_box, tol=1.0):\n",
    "    ix0, iy0, ix1, iy1 = inner_box\n",
    "    ox0, oy0, ox1, oy1 = outer_box\n",
    "    return (ox0 - tol) <= ix0 and (oy0 - tol) <= iy0 and (ox1 + tol) >= ix1 and (oy1 + tol) >= iy1\n",
    "\n",
    "def build_page_index(blocks):\n",
    "    page_map = defaultdict(list)\n",
    "    for b in blocks:\n",
    "        if b[\"page_idx\"] is not None:\n",
    "            page_map[b[\"page_idx\"]].append(b)\n",
    "    for pg in page_map:\n",
    "        page_map[pg].sort(key=lambda bl: (bl[\"bbox\"][1] if bl[\"bbox\"] else 0,\n",
    "                                          bl[\"bbox\"][0] if bl[\"bbox\"] else 0))\n",
    "    return page_map\n",
    "\n",
    "def nearest_text_block_idx(page_blocks, page0, link_bbox):\n",
    "    best_idx, best_area = None, float(\"inf\")\n",
    "    for b in page_blocks.get(page0, []):\n",
    "        if b[\"type\"] != \"text\" or not b.get(\"bbox\"):\n",
    "            continue\n",
    "        if is_contained(link_bbox, b[\"bbox\"]):\n",
    "            area = (b[\"bbox\"][2]-b[\"bbox\"][0]) * (b[\"bbox\"][3]-b[\"bbox\"][1])\n",
    "            if area < best_area:\n",
    "                best_area = area\n",
    "                best_idx = b[\"block_idx\"]\n",
    "    return best_idx\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) block_idx â†’ clause\n",
    "# ------------------------------------------------------------\n",
    "def clause_of_block(spans, block_idx):\n",
    "    if block_idx is None:\n",
    "        return None, \"\"\n",
    "    for sp in spans:\n",
    "        if sp[\"start_block\"] == block_idx or (sp[\"start_block\"] < block_idx < sp[\"end_block\"]):\n",
    "            return sp[\"clause_id\"], sp[\"full_text\"]   # â˜… å›å‚³æ–‡å­—\n",
    "    return None, \"\"\n",
    "\n",
    "def extract_doc_id_from_uri(uri: str) -> str:\n",
    "    if uri.endswith(\".pdf\"):\n",
    "        # æå–æœ€å¾Œä¸€æ®µè·¯å¾‘ï¼Œå»æ‰ \".pdf\"\n",
    "        return uri.split(\"/\")[-1].replace(\".pdf\", \"\")\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "938bf0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… saved: C:\\Users\\User\\Desktop\\GraphRAG\\Main_Workflow\\Hyperlink_extract\\AML-1_hyperlink_matches_with_clause.csv  rows=6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>uri</th>\n",
       "      <th>normalized_link_bbox</th>\n",
       "      <th>matched_block_idx</th>\n",
       "      <th>clause_id</th>\n",
       "      <th>clause_text</th>\n",
       "      <th>referenced_doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>http://www.hkma.gov.hk/media/eng/doc/key-funct...</td>\n",
       "      <td>[624.68, 224.95, 737.54, 241.35]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>http://www.hkma.gov.hk/media/eng/doc/key-funct...</td>\n",
       "      <td>[139.06, 241.35, 226.12, 257.75]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>GL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>https://www.hkma.gov.hk/eng/key-functions/bank...</td>\n",
       "      <td>[191.05, 439.64, 279.27, 470.3]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>http://www.hkma.gov.hk/media/eng/doc/key-funct...</td>\n",
       "      <td>[367.68, 439.64, 454.74, 470.3]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>GL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>https://www.hkma.gov.hk/eng/index.shtml</td>\n",
       "      <td>[543.92, 439.64, 605.23, 470.3]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23</td>\n",
       "      <td>http://www.hkma.gov.hk/media/eng/doc/key-funct...</td>\n",
       "      <td>[700.02, 439.64, 812.89, 470.3]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page                                                uri  \\\n",
       "0     1  http://www.hkma.gov.hk/media/eng/doc/key-funct...   \n",
       "1     1  http://www.hkma.gov.hk/media/eng/doc/key-funct...   \n",
       "2    23  https://www.hkma.gov.hk/eng/key-functions/bank...   \n",
       "3    23  http://www.hkma.gov.hk/media/eng/doc/key-funct...   \n",
       "4    23            https://www.hkma.gov.hk/eng/index.shtml   \n",
       "5    23  http://www.hkma.gov.hk/media/eng/doc/key-funct...   \n",
       "\n",
       "               normalized_link_bbox  matched_block_idx clause_id clause_text  \\\n",
       "0  [624.68, 224.95, 737.54, 241.35]                1.0      None               \n",
       "1  [139.06, 241.35, 226.12, 257.75]                NaN      None               \n",
       "2   [191.05, 439.64, 279.27, 470.3]                NaN      None               \n",
       "3   [367.68, 439.64, 454.74, 470.3]                NaN      None               \n",
       "4   [543.92, 439.64, 605.23, 470.3]                NaN      None               \n",
       "5   [700.02, 439.64, 812.89, 470.3]                NaN      None               \n",
       "\n",
       "  referenced_doc_id  \n",
       "0                IN  \n",
       "1                GL  \n",
       "2                    \n",
       "3                GL  \n",
       "4                    \n",
       "5                IN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 6) ä¸»æµç¨‹\n",
    "# ------------------------------------------------------------\n",
    "def run_matching(content_list_path, links_csv_path, output_csv_path):\n",
    "    blocks = load_blocks(content_list_path)\n",
    "    heads  = find_headings(blocks)\n",
    "    spans  = build_spans_dedup(blocks, heads)\n",
    "    page_blocks = build_page_index(blocks)\n",
    "\n",
    "    links_df = pd.read_csv(links_csv_path)\n",
    "    out_rows = []\n",
    "\n",
    "    for _, row in links_df.iterrows():\n",
    "        page0 = int(row[\"page\"])\n",
    "        rect  = parse_rect_string(row[\"rect\"])\n",
    "        if not rect:\n",
    "            continue\n",
    "        link_bb = normalize_rect(rect, float(row[\"page_width\"]), float(row[\"page_height\"]))\n",
    "        blk_idx = nearest_text_block_idx(page_blocks, page0, link_bb)\n",
    "        cid, clause_text = clause_of_block(spans, blk_idx)\n",
    "\n",
    "        uri = row[\"uri\"]\n",
    "        ref_doc_id = extract_doc_id_from_uri(uri)\n",
    "\n",
    "        out_rows.append({\n",
    "            \"page\": page0 + 1,\n",
    "            \"uri\": row[\"uri\"],\n",
    "            \"normalized_link_bbox\": [round(c,2) for c in link_bb],\n",
    "            \"matched_block_idx\": blk_idx,\n",
    "            \"clause_id\": cid,\n",
    "            \"clause_text\": clause_text,\n",
    "            \"referenced_doc_id\": ref_doc_id\n",
    "        })\n",
    "\n",
    "    out_df = pd.DataFrame(out_rows)\n",
    "    out_df.to_csv(output_csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… saved: {output_csv_path}  rows={len(out_df)}\")\n",
    "    return out_df\n",
    "\n",
    "run_matching(CONTENT_LIST_JSON, INPUT_LINKS_CSV, OUTPUT_MATCH_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faac4307",
   "metadata": {},
   "source": [
    "# DownLoad PDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e56a96a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ä¸‹è¼‰è³‡æ–™å¤¾å·²æº–å‚™å¥½: c:\\Users\\User\\Desktop\\GraphRAG\\Main_Workflow\\downloaded_pdfs\\AML-1\n",
      "ğŸ”— æ‰¾åˆ° 2 å€‹ä¸é‡è¤‡çš„ PDF é€£çµæº–å‚™ä¸‹è¼‰ã€‚\n",
      "â³ æ­£åœ¨ä¸‹è¼‰: http://www.hkma.gov.hk/media/eng/doc/key-functions/banking-stability/supervisory-policy-manual/IN.pdf\n",
      "âœ… ä¸‹è¼‰æˆåŠŸ: IN.pdf\n",
      "â³ æ­£åœ¨ä¸‹è¼‰: http://www.hkma.gov.hk/media/eng/doc/key-functions/banking-stability/supervisory-policy-manual/GL.pdf\n",
      "âœ… ä¸‹è¼‰æˆåŠŸ: GL.pdf\n",
      "\n",
      "--- æ‰€æœ‰ä¸‹è¼‰ä»»å‹™å·²å®Œæˆ ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "\n",
    "# å¢åŠ é€™è¡Œä¾†æŠ‘åˆ¶ SSL é©—è­‰çš„è­¦å‘Šè¨Šæ¯\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "def download_pdfs_from_csv(csv_path: Path, download_dir: Path):\n",
    "    \"\"\"\n",
    "    å¾ CSV æª”æ¡ˆä¸­è®€å– URIï¼Œä¸¦è‡ªå‹•ä¸‹è¼‰å…¶ä¸­çš„ PDF æª”æ¡ˆã€‚\n",
    "\n",
    "    Args:\n",
    "        csv_path (Path): åŒ…å« 'uri' æ¬„ä½çš„ CSV æª”æ¡ˆè·¯å¾‘ã€‚\n",
    "        download_dir (Path): ç”¨æ–¼å„²å­˜ä¸‹è¼‰çš„ PDF çš„è³‡æ–™å¤¾è·¯å¾‘ã€‚\n",
    "    \"\"\"\n",
    "    # 1. ç¢ºä¿ä¸‹è¼‰è³‡æ–™å¤¾å­˜åœ¨\n",
    "    download_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"ğŸ“ ä¸‹è¼‰è³‡æ–™å¤¾å·²æº–å‚™å¥½: {download_dir}\")\n",
    "\n",
    "    # 2. è®€å– CSV æª”æ¡ˆ\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if 'uri' not in df.columns:\n",
    "            print(f\"âŒ éŒ¯èª¤: CSV æª”æ¡ˆ '{csv_path}' ä¸­æ‰¾ä¸åˆ° 'uri' æ¬„ä½ã€‚\")\n",
    "            return\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ° CSV æª”æ¡ˆ '{csv_path}'ã€‚\")\n",
    "        return\n",
    "\n",
    "    # 3. ç²å–æ‰€æœ‰ä¸é‡è¤‡çš„ã€ä»¥ .pdf çµå°¾çš„ URL\n",
    "    urls_to_download = df['uri'].dropna().unique()\n",
    "    pdf_urls = [url for url in urls_to_download if url.lower().endswith('.pdf')]\n",
    "    \n",
    "    if not pdf_urls:\n",
    "        print(\"â„¹ï¸ åœ¨ CSV ä¸­æ²’æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„ PDF é€£çµã€‚\")\n",
    "        return\n",
    "\n",
    "    print(f\"ğŸ”— æ‰¾åˆ° {len(pdf_urls)} å€‹ä¸é‡è¤‡çš„ PDF é€£çµæº–å‚™ä¸‹è¼‰ã€‚\")\n",
    "    \n",
    "    # 4. éæ­·ä¸¦ä¸‹è¼‰æ¯ä¸€å€‹ PDF\n",
    "    for url in pdf_urls:\n",
    "        try:\n",
    "            # å¾ URL ä¸­æå–æª”æ¡ˆåç¨±\n",
    "            filename = os.path.basename(urlparse(url).path)\n",
    "            if not filename: # å¦‚æœ URL ä»¥ / çµå°¾ï¼Œæª”åå¯èƒ½ç‚ºç©º\n",
    "                filename = \"downloaded_file.pdf\"\n",
    "            save_path = download_dir / filename\n",
    "\n",
    "            # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨ï¼Œé¿å…é‡è¤‡ä¸‹è¼‰\n",
    "            if save_path.exists():\n",
    "                print(f\"â­ï¸  å·²è·³é (æª”æ¡ˆå·²å­˜åœ¨): {filename}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"â³ æ­£åœ¨ä¸‹è¼‰: {url}\")\n",
    "            \n",
    "            # ç™¼é€ GET è«‹æ±‚ï¼Œè¨­å®šè¶…æ™‚ä¸¦å¢åŠ  verify=False ä¾†å¿½ç•¥ SSL éŒ¯èª¤\n",
    "            response = requests.get(url, stream=True, timeout=20, verify=False)\n",
    "            response.raise_for_status()  # å¦‚æœè«‹æ±‚å¤±æ•— (å¦‚ 404)ï¼Œå‰‡æœƒå¼•ç™¼éŒ¯èª¤\n",
    "\n",
    "            # å°‡å…§å®¹å¯«å…¥æª”æ¡ˆ\n",
    "            with open(save_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            \n",
    "            print(f\"âœ… ä¸‹è¼‰æˆåŠŸ: {filename}\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"âŒ ä¸‹è¼‰å¤±æ•—: {url} - éŒ¯èª¤: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è™•ç†å¤±æ•—: {url} - ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}\")\n",
    "\n",
    "    print(\"\\n--- æ‰€æœ‰ä¸‹è¼‰ä»»å‹™å·²å®Œæˆ ---\")\n",
    "\n",
    "\n",
    "# --- ä¸»åŸ·è¡Œå€å¡Š ---\n",
    "\n",
    "# --- è«‹åœ¨é€™è£¡é…ç½®ä½ çš„è¼¸å…¥å’Œè¼¸å‡ºè·¯å¾‘ ---\n",
    "DOC_CODE = \"AML-1\"\n",
    "\n",
    "# è¼¸å…¥çš„ CSV æª”æ¡ˆè·¯å¾‘ (è«‹ç¢ºä¿é€™å€‹æª”æ¡ˆå­˜åœ¨)\n",
    "INPUT_MATCHES_CSV = Path(f\"c:/Users/User/Desktop/GraphRAG/Main_Workflow/Hyperlink_extract/{DOC_CODE}.links_normalized.csv\")\n",
    "\n",
    "# ä½ å¸Œæœ›å°‡ PDF ä¸‹è¼‰åˆ°å“ªå€‹è³‡æ–™å¤¾\n",
    "PDF_DOWNLOAD_DIR = Path(f\"c:/Users/User/Desktop/GraphRAG/Main_Workflow/downloaded_pdfs/{DOC_CODE}\")\n",
    "\n",
    "# åŸ·è¡Œä¸‹è¼‰å‡½æ•¸\n",
    "# åœ¨é‹è¡Œå‰ï¼Œè«‹ç¢ºä¿ä½ å·²ç¶“å®‰è£äº† requests å‡½å¼åº« (pip install requests)\n",
    "download_pdfs_from_csv(\n",
    "    csv_path=INPUT_MATCHES_CSV,\n",
    "    download_dir=PDF_DOWNLOAD_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb9d489",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
